{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19ah5rFXj-Hv6WRVtK3voCrm3l5pkUAju","timestamp":1744282450969},{"file_id":"1Np8NjhFegXY_BxHZppo2Z2g1uPH18UsB","timestamp":1733808878364},{"file_id":"1zbAwvetsq_w3Me_S47QC9QKdfyF9pMOS","timestamp":1733808327761}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"id":"zGp9oN3fzz56","executionInfo":{"status":"ok","timestamp":1744290812367,"user_tz":240,"elapsed":8,"user":{"displayName":"arnab bandyopadhyay","userId":"03820230275428411305"}}},"outputs":[],"source":["import numpy as np\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","\n","from sklearn.metrics import accuracy_score\n","\n","\n","import warnings\n","\n","# Ignore all warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","source":["**Question 11**"],"metadata":{"id":"tg-h6dNnCBFa"}},{"cell_type":"code","source":["import math\n","# Define the Sigmoid function\n","def sigmoid(x):\n","    \"\"\"\n","    Applies the Sigmoid function element-wise.\n","    Sigmoid(x) = 1 / (1 + exp(-x))\n","    \"\"\"\n","    return (1 / (1 + np.exp(-x)))  # write your code here\n","\n","# Define the derivative of the Sigmoid function\n","def sigmoid_derivative(x):\n","    \"\"\"\n","    Computes the derivative of the Sigmoid function.\n","    Sigmoid'(x) = Sigmoid(x) * (1 - Sigmoid(x))\n","    \"\"\"\n","    s = sigmoid(x) * (1 - sigmoid(x)) # write your code here\n","    return s # write your code here\n","\n","# Define the ReLU function\n","def relu(x):\n","    \"\"\"\n","    Applies the ReLU function element-wise.\n","    ReLU(x) = max(0, x)\n","    \"\"\"\n","    return np.maximum(0,x) # write your code here\n","\n","# Define the derivative of the ReLU function\n","def relu_derivative(x):\n","    \"\"\"\n","    Computes the derivative of the ReLU function.\n","    ReLU'(x) = 1 if x > 0, else 0\n","    \"\"\"\n","    return np.where(x > 0, 1, 0) # write your code here\n","\n","# Example usage\n","\n","# Example input\n","x = np.array([-2, -1, 0, 1, 2], dtype=float)\n","\n","# Apply Sigmoid\n","sigmoid_result = sigmoid(x)\n","print(\"Sigmoid:\", sigmoid_result)\n","\n","# Compute the derivative\n","sigmoid_derivative_result = sigmoid_derivative(x)\n","print(\"Sigmoid Derivative:\", sigmoid_derivative_result)\n","\n","# Apply ReLU\n","relu_result = relu(x)\n","print(\"ReLU:\", relu_result)\n","\n","# Compute the derivative\n","relu_derivative_result = relu_derivative(x)\n","print(\"ReLU Derivative:\", relu_derivative_result)"],"metadata":{"id":"Rm9D27zDEFxb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744290815249,"user_tz":240,"elapsed":30,"user":{"displayName":"arnab bandyopadhyay","userId":"03820230275428411305"}},"outputId":"026db849-13e9-4b24-9fcd-fdf3c925769c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n","Sigmoid Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n","ReLU: [0. 0. 0. 1. 2.]\n","ReLU Derivative: [0 0 0 1 1]\n"]}]},{"cell_type":"markdown","source":["**Question 12**"],"metadata":{"id":"TlexKUBNkgcJ"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Mean Squared Error Loss\n","def mean_squared_error(y_true, y_pred):\n","    \"\"\"\n","    Computes Mean Squared Error (MSE) loss.\n","    \"\"\"\n","    return (y_pred - np.mean(y_true))**2 / len(y_pred)  # write your code here\n","\n","# Binary Cross-Entropy Loss\n","def binary_cross_entropy(y_true, y_pred):\n","    \"\"\"\n","    Computes Binary Cross-Entropy (BCE) loss.\n","    \"\"\"\n","    epsilon = 1e-8  # To avoid log(0)\n","    y_pred = y_true * math.log(y_pred) + (1-y_true) * math.log(1 - y_pred) # write your code here  # Ensure y_pred stays in (0,1)\n","    return y_pred  # write your code here\n","\n","\n","# Example Usage\n","\n","# Example data\n","y_true = np.array([1, 0, 1, 0], dtype=float)  # Ground truth\n","y_pred_mse = np.array([0.8, 0.2, 0.9, 0.3], dtype=float)  # Predictions (MSE)\n","y_pred_bce = np.array([0.8, 0.2, 0.9, 0.3], dtype=float)  # Predictions (BCE)\n","\n","# MSE loss\n","mse_loss = mean_squared_error(y_true, y_pred_mse)\n","print(\"MSE Loss:\", mse_loss)\n","\n","# BCE loss\n","bce_loss = binary_cross_entropy(y_true, y_pred_bce)\n","print(\"BCE Loss:\", bce_loss)"],"metadata":{"id":"tu7syipn0pHW","colab":{"base_uri":"https://localhost:8080/","height":348},"executionInfo":{"status":"error","timestamp":1744299072406,"user_tz":240,"elapsed":198,"user":{"displayName":"arnab bandyopadhyay","userId":"03820230275428411305"}},"outputId":"b25c5bd5-1019-4b96-ddd0-6337efa058cd"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE Loss: [0.0225 0.0225 0.04   0.01  ]\n"]},{"output_type":"error","ename":"TypeError","evalue":"only length-1 arrays can be converted to Python scalars","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5c45d5673616>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# BCE loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mbce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_bce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BCE Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-5c45d5673616>\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-8\u001b[0m  \u001b[0;31m# To avoid log(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# write your code here  # Ensure y_pred stays in (0,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m  \u001b[0;31m# write your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"]}]},{"cell_type":"markdown","source":["**Question 13**"],"metadata":{"id":"eWMJEwl52dEQ"}},{"cell_type":"code","source":["# Generate synthetic data\n","X, y = make_classification(\n","    n_samples=1000, n_features=5, n_informative=3, n_redundant=0, random_state=42\n",")\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = # write your code here\n","\n","# Train the model with sigmoid activation\n","model_sigmoid = # write your code here\n","model_sigmoid.fit(X_train, y_train)\n","y_pred_sigmoid = # write your code here\n","accuracy_sigmoid = # write your code here\n","\n","# Train the model with tanh activation\n","model_tanh = # write your code here\n","model_tanh.fit(X_train, y_train)\n","y_pred_tanh = # write your code here\n","accuracy_tanh = # write your code here\n","\n","# Print the results\n","print(f\"Accuracy with Sigmoid activation: {accuracy_sigmoid:.4f}\")\n","print(f\"Accuracy with Tanh activation: {accuracy_tanh:.4f}\")"],"metadata":{"id":"AfZ5gvC51Uyi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question 14**"],"metadata":{"id":"FOZfanDDaTpz"}},{"cell_type":"code","source":["# Generate synthetic data\n","X, y = make_classification(\n","    n_samples=100, n_features=10, n_informative=8, n_redundant=2, random_state=42\n",")\n","\n","# Split into training and validation sets\n","X_train, X_val, y_train, y_val = # write your code here\n","\n","# Normalize the data\n","X_train = # write your code here\n","X_val = # write your code here\n","\n","# Overfitting model\n","def build_overfitting_model():\n","    model = Sequential([\n","        Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n","        Dense(256, activation='relu'),\n","        Dense(128, activation='relu'),\n","        Dense(64, activation='relu'),\n","        Dense(32, activation='relu'),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Model with dropout to prevent overfitting\n","def build_dropout_model():\n","    model = Sequential([\n","        Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n","        Dropout(0.5),\n","        Dense(256, activation='relu'),\n","        Dropout(0.5),\n","        Dense(128, activation='relu'),\n","        Dropout(0.5),\n","        Dense(64, activation='relu'),\n","        Dropout(0.5),\n","        Dense(32, activation='relu'),\n","        Dropout(0.5),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Train overfitting model\n","print(\"Training overfitting model...\")\n","overfitting_model = build_overfitting_model()\n","history_overfit = overfitting_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n","\n","# Train dropout model\n","print(\"Training model with dropout...\")\n","dropout_model = build_dropout_model()\n","history_dropout = dropout_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n","\n","# Evaluate models\n","overfit_val_acc = overfitting_model.evaluate(X_val, y_val, verbose=0)[1]\n","dropout_val_acc = dropout_model.evaluate(X_val, y_val, verbose=0)[1]\n","\n","print(f\"Validation Accuracy before dropout: {overfit_val_acc:.2f}\")\n","print(f\"Validation Accuracy after dropout: {dropout_val_acc:.2f}\")"],"metadata":{"id":"tRJ5XZ9hgpU0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question 15**"],"metadata":{"id":"QsNzlVYS0wry"}},{"cell_type":"code","source":["# Sigmoid activation function\n","def sigmoid(z):\n","    return # write your code here\n","\n","# Derivative of the sigmoid function\n","def sigmoid_derivative(z):\n","    return # write your code here\n","\n","# Binary Cross-Entropy loss function\n","def binary_cross_entropy(y_true, y_pred):\n","    return # write your code here\n","\n","# Neural Network class\n","class SimpleNN:\n","    def __init__(self, input_dim):\n","        # Initialize weights and bias\n","        self.weights = np.random.randn(input_dim) * 0.01\n","        self.bias = 0.0\n","\n","    def forward(self, X):\n","        # Linear combination\n","        z = # write your code here\n","        # Apply sigmoid activation\n","        y_pred = # write your code here\n","        return y_pred\n","\n","    def backward(self, X, y_true, y_pred, learning_rate):\n","        # Compute gradients\n","        m = X.shape[0]\n","        dz = y_pred - y_true\n","        dw = np.dot(X.T, dz) / m\n","        db = np.sum(dz) / m\n","\n","        # Update weights and bias\n","        self.weights -= learning_rate * dw\n","        self.bias -= learning_rate * db\n","\n","    def train(self, X, y_true, epochs, learning_rate):\n","        for epoch in range(epochs):\n","            # Forward pass\n","            y_pred = self.forward(X)\n","            # Compute loss\n","            loss = binary_cross_entropy(y_true, y_pred)\n","            # Backward pass\n","            self.backward(X, y_true, y_pred, learning_rate)\n","\n","            # Print loss every 100 epochs\n","            if epoch % 100 == 0:\n","                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n","\n","# Example usage\n","\n","# Generate synthetic data\n","np.random.seed(42)\n","X = np.random.rand(100, 2)  # 100 samples, 2 features\n","y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Simple decision boundary\n","\n","# Create and train the neural network\n","nn = SimpleNN(input_dim=2)\n","nn.train(X, y, epochs=1000, learning_rate=0.1)\n","\n","# Test the model\n","test_sample = np.array([[0.7, 0.6]])\n","prediction = nn.forward(test_sample)\n","print(f\"Prediction for {test_sample}: class {1 if prediction > 0.5 else 0}\")"],"metadata":{"id":"k_iyNzmF1vp6"},"execution_count":null,"outputs":[]}]}